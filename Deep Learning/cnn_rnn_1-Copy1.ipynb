{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "#from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "#from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/home/datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/home/datasets/Project_data/val.csv').readlines())\n",
    "batch_size = 30 #experiment with the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_process(image):\n",
    "    if (image.shape[0] == 120):\n",
    "        crop_image    = image[:,20:140]\n",
    "        resized_image = cv2.resize(crop_image, (120,120), interpolation = cv2.INTER_AREA)\n",
    "    if (image.shape[0] == 360):\n",
    "        crop_image = image[50:310:,50:310]\n",
    "        resized_image = cv2.resize(crop_image, (120,120), interpolation = cv2.INTER_AREA)        \n",
    "\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = range(30) #create a list of image numbers you want to use for a particular video\n",
    "    #img_idx = [5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
    "    i = 1\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size  # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches            \n",
    "            x = len(img_idx)\n",
    "            y = 120\n",
    "            z = 120\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            \n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    #we first resize and then crop image\n",
    "                    image = image_process(image)\n",
    "                          \n",
    "\n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0] - image[:,:,0].min())/(image[:,:,0].max() - image[:,:,0].min())  #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1] - image[:,:,1].min())/(image[:,:,1].max() - image[:,:,1].min())  #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2] - image[:,:,2].min())/(image[:,:,2].max() - image[:,:,2].min())  #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # Code for the remaining data points which are left after full batches\n",
    "\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            x = len(img_idx)\n",
    "            y = 120\n",
    "            z = 120\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            \n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    #image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    #we first resize and then crop image\n",
    "                    image = image_process(image)                                            \n",
    "                    #normalize image\n",
    "                    batch_data[folder,idx,:,:,0] = (image[:,:,0] - image[:,:,0].min())/(image[:,:,0].max() - image[:,:,0].min())  #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:,:,1] - image[:,:,1].min())/(image[:,:,1].max() - image[:,:,1].min())  #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:,:,2] - image[:,:,2].min())/(image[:,:,2].max() - image[:,:,2].min())  #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 30\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/home/datasets/Project_data/train'\n",
    "val_path = '/home/datasets/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 30 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2 using CNN+RNN\n",
    "#import libraries\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation , Dropout\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D ,MaxPooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import TimeDistributed,GRU,Conv2D\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet = ResNet50(include_top=False,weights='imagenet',input_shape=(120,120,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for layer in resnet.layers[:-10]:\n",
    "#    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#cnn = Sequential([resnet])\n",
    "#cnn.add(Conv2D(64,(3,3)))\n",
    "#cnn.add(Conv2D(128,(2,2)))\n",
    "#cnn.add(Flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model = Sequential()\n",
    "#model.add(TimeDistributed(cnn,input_shape=(30,120,120,3)))\n",
    "#model.add(TimeDistributed(BatchNormalization()))\n",
    "#model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "#model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "#model.add(GRU(128,return_sequences=True))\n",
    "#model.add(GRU(128))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "#model.add(Dense(256,activation='relu'))\n",
    "#model.add(Dense(256,activation='relu'))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "#model.add(Dense(5, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import mobilenet\n",
    "pretrained_mobilenet = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(TimeDistributed(pretrained_mobilenet,input_shape=(30,120,120,3)))\n",
    "\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(GRU(128))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed (TimeDistri (None, 30, 3, 3, 1024)    3228864   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 30, 3, 3, 1024)    4096      \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 30, 1, 1, 1024)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 30, 1024)          0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 128)               443136    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 3,693,253\n",
      "Trainable params: 3,669,317\n",
      "Non-trainable params: 23,936\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam(lr=0.0001) #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n",
    "\n",
    "#LR = reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,patience=3, min_lr=0.00001) # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-bd77c9c60c14>:3: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Source path =  /home/datasets/Project_data/train ; batch size = 30\n",
      "Epoch 1/30\n",
      "21/23 [==========================>...] - ETA: 4s - loss: 1.7280 - categorical_accuracy: 0.2698Batch:  23 Index: 30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7198 - categorical_accuracy: 0.2760Source path =  /home/datasets/Project_data/val ; batch size = 30\n",
      "Batch:  4 Index: 30\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-06-2717_35_34.791526/model-00001-1.71983-0.27602-1.37463-0.43000.h5\n",
      "23/23 [==============================] - 62s 3s/step - loss: 1.7198 - categorical_accuracy: 0.2760 - val_loss: 1.3746 - val_categorical_accuracy: 0.4300\n",
      "Epoch 2/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.2680 - categorical_accuracy: 0.4928\n",
      "Epoch 00002: saving model to model_init_2021-06-2717_35_34.791526/model-00002-1.26804-0.49275-1.22278-0.42500.h5\n",
      "23/23 [==============================] - 11s 469ms/step - loss: 1.2680 - categorical_accuracy: 0.4928 - val_loss: 1.2228 - val_categorical_accuracy: 0.4250\n",
      "Epoch 3/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.2046 - categorical_accuracy: 0.5362\n",
      "Epoch 00003: saving model to model_init_2021-06-2717_35_34.791526/model-00003-1.20457-0.53623-1.00264-0.65000.h5\n",
      "23/23 [==============================] - 10s 442ms/step - loss: 1.2046 - categorical_accuracy: 0.5362 - val_loss: 1.0026 - val_categorical_accuracy: 0.6500\n",
      "Epoch 4/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.2294 - categorical_accuracy: 0.4203\n",
      "Epoch 00004: saving model to model_init_2021-06-2717_35_34.791526/model-00004-1.22935-0.42029-1.00358-0.70000.h5\n",
      "23/23 [==============================] - 10s 440ms/step - loss: 1.2294 - categorical_accuracy: 0.4203 - val_loss: 1.0036 - val_categorical_accuracy: 0.7000\n",
      "Epoch 5/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.1877 - categorical_accuracy: 0.5507\n",
      "Epoch 00005: saving model to model_init_2021-06-2717_35_34.791526/model-00005-1.18768-0.55072-0.91463-0.65000.h5\n",
      "23/23 [==============================] - 11s 480ms/step - loss: 1.1877 - categorical_accuracy: 0.5507 - val_loss: 0.9146 - val_categorical_accuracy: 0.6500\n",
      "Epoch 6/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.1778 - categorical_accuracy: 0.4638\n",
      "Epoch 00006: saving model to model_init_2021-06-2717_35_34.791526/model-00006-1.17784-0.46377-0.79608-0.75000.h5\n",
      "23/23 [==============================] - 10s 434ms/step - loss: 1.1778 - categorical_accuracy: 0.4638 - val_loss: 0.7961 - val_categorical_accuracy: 0.7500\n",
      "Epoch 7/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.0304 - categorical_accuracy: 0.5652\n",
      "Epoch 00007: saving model to model_init_2021-06-2717_35_34.791526/model-00007-1.03041-0.56522-0.74306-0.75000.h5\n",
      "23/23 [==============================] - 10s 416ms/step - loss: 1.0304 - categorical_accuracy: 0.5652 - val_loss: 0.7431 - val_categorical_accuracy: 0.7500\n",
      "Epoch 8/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.0101 - categorical_accuracy: 0.5942\n",
      "Epoch 00008: saving model to model_init_2021-06-2717_35_34.791526/model-00008-1.01008-0.59420-0.88465-0.57500.h5\n",
      "23/23 [==============================] - 11s 472ms/step - loss: 1.0101 - categorical_accuracy: 0.5942 - val_loss: 0.8847 - val_categorical_accuracy: 0.5750\n",
      "Epoch 9/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.0698 - categorical_accuracy: 0.6232\n",
      "Epoch 00009: saving model to model_init_2021-06-2717_35_34.791526/model-00009-1.06985-0.62319-0.66389-0.80000.h5\n",
      "23/23 [==============================] - 10s 442ms/step - loss: 1.0698 - categorical_accuracy: 0.6232 - val_loss: 0.6639 - val_categorical_accuracy: 0.8000\n",
      "Epoch 10/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9788 - categorical_accuracy: 0.6667\n",
      "Epoch 00010: saving model to model_init_2021-06-2717_35_34.791526/model-00010-0.97878-0.66667-0.81524-0.67500.h5\n",
      "23/23 [==============================] - 10s 417ms/step - loss: 0.9788 - categorical_accuracy: 0.6667 - val_loss: 0.8152 - val_categorical_accuracy: 0.6750\n",
      "Epoch 11/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7317 - categorical_accuracy: 0.7391\n",
      "Epoch 00011: saving model to model_init_2021-06-2717_35_34.791526/model-00011-0.73173-0.73913-0.63676-0.82500.h5\n",
      "23/23 [==============================] - 10s 414ms/step - loss: 0.7317 - categorical_accuracy: 0.7391 - val_loss: 0.6368 - val_categorical_accuracy: 0.8250\n",
      "Epoch 12/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7114 - categorical_accuracy: 0.7681\n",
      "Epoch 00012: saving model to model_init_2021-06-2717_35_34.791526/model-00012-0.71144-0.76812-0.74474-0.72500.h5\n",
      "23/23 [==============================] - 10s 439ms/step - loss: 0.7114 - categorical_accuracy: 0.7681 - val_loss: 0.7447 - val_categorical_accuracy: 0.7250\n",
      "Epoch 13/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.8993 - categorical_accuracy: 0.6377\n",
      "Epoch 00013: saving model to model_init_2021-06-2717_35_34.791526/model-00013-0.89927-0.63768-0.66231-0.75000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "23/23 [==============================] - 11s 487ms/step - loss: 0.8993 - categorical_accuracy: 0.6377 - val_loss: 0.6623 - val_categorical_accuracy: 0.7500\n",
      "Epoch 14/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.7608 - categorical_accuracy: 0.7391\n",
      "Epoch 00014: saving model to model_init_2021-06-2717_35_34.791526/model-00014-0.76083-0.73913-0.57017-0.82500.h5\n",
      "23/23 [==============================] - 11s 458ms/step - loss: 0.7608 - categorical_accuracy: 0.7391 - val_loss: 0.5702 - val_categorical_accuracy: 0.8250\n",
      "Epoch 15/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6917 - categorical_accuracy: 0.8116\n",
      "Epoch 00015: saving model to model_init_2021-06-2717_35_34.791526/model-00015-0.69166-0.81159-0.55568-0.80000.h5\n",
      "23/23 [==============================] - 10s 425ms/step - loss: 0.6917 - categorical_accuracy: 0.8116 - val_loss: 0.5557 - val_categorical_accuracy: 0.8000\n",
      "Epoch 16/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6782 - categorical_accuracy: 0.7826\n",
      "Epoch 00016: saving model to model_init_2021-06-2717_35_34.791526/model-00016-0.67823-0.78261-0.54680-0.80000.h5\n",
      "23/23 [==============================] - 11s 465ms/step - loss: 0.6782 - categorical_accuracy: 0.7826 - val_loss: 0.5468 - val_categorical_accuracy: 0.8000\n",
      "Epoch 17/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5466 - categorical_accuracy: 0.8406\n",
      "Epoch 00017: saving model to model_init_2021-06-2717_35_34.791526/model-00017-0.54656-0.84058-0.50930-0.80000.h5\n",
      "23/23 [==============================] - 9s 399ms/step - loss: 0.5466 - categorical_accuracy: 0.8406 - val_loss: 0.5093 - val_categorical_accuracy: 0.8000\n",
      "Epoch 18/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6462 - categorical_accuracy: 0.7826\n",
      "Epoch 00018: saving model to model_init_2021-06-2717_35_34.791526/model-00018-0.64618-0.78261-0.63948-0.77500.h5\n",
      "23/23 [==============================] - 10s 427ms/step - loss: 0.6462 - categorical_accuracy: 0.7826 - val_loss: 0.6395 - val_categorical_accuracy: 0.7750\n",
      "Epoch 19/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5416 - categorical_accuracy: 0.8841\n",
      "Epoch 00019: saving model to model_init_2021-06-2717_35_34.791526/model-00019-0.54161-0.88406-0.48375-0.80000.h5\n",
      "23/23 [==============================] - 10s 416ms/step - loss: 0.5416 - categorical_accuracy: 0.8841 - val_loss: 0.4837 - val_categorical_accuracy: 0.8000\n",
      "Epoch 20/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5275 - categorical_accuracy: 0.8116\n",
      "Epoch 00020: saving model to model_init_2021-06-2717_35_34.791526/model-00020-0.52746-0.81159-0.61009-0.75000.h5\n",
      "23/23 [==============================] - 11s 468ms/step - loss: 0.5275 - categorical_accuracy: 0.8116 - val_loss: 0.6101 - val_categorical_accuracy: 0.7500\n",
      "Epoch 21/30\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.6676 - categorical_accuracy: 0.7681\n",
      "Epoch 00021: saving model to model_init_2021-06-2717_35_34.791526/model-00021-0.66763-0.76812-0.50178-0.80000.h5\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "23/23 [==============================] - 10s 456ms/step - loss: 0.6676 - categorical_accuracy: 0.7681 - val_loss: 0.5018 - val_categorical_accuracy: 0.8000\n",
      "Epoch 22/30\n",
      "16/23 [===================>..........] - ETA: 1s - loss: 0.3438 - categorical_accuracy: 0.9792"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
